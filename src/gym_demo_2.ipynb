{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "import src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"floatX=float32\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%env THEANO_FLAGS=\"floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAME = \"FrontoPolarStocks-v0\"\n",
    "\n",
    "N_AGENTS = 1\n",
    "SEQ_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-18 22:32:15,463] Making new env: FrontoPolarStocks-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30.510001  78.719998  18.49      44.48      48.360001  10.14      21.4\n",
      "  49.5       18.540001  45.240002  27.459999  26.93      13.21      11.02    ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(GAME)\n",
    "env.reset()\n",
    "\n",
    "action_shape = (env.action_space.num_discrete_space, 3)\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "state, _, _, _ = env.step([0] * action_shape[0])\n",
    "\n",
    "action_names = np.array([\"sell\", \"pass\", \"buy\"]) #i guess so... i may be wrong\n",
    "\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using shallow neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agent import build_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent, action_layer, V_layer = build_agent(action_shape, state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_layers[0].output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((action_layer,V_layer),trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-18 22:32:17,931] Making new env: FrontoPolarStocks-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(agent, GAME, N_AGENTS, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy'\n",
      "   'buy' 'buy']\n",
      "  ['buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy'\n",
      "   'buy' 'buy']\n",
      "  ['buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy'\n",
      "   'buy' 'buy']\n",
      "  ['buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy'\n",
      "   'buy' 'buy']\n",
      "  ['buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy'\n",
      "   'buy' 'buy']\n",
      "  ['buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy' 'buy'\n",
      "   'buy' 'buy']\n",
      "  ['pass' 'buy' 'pass' 'pass' 'pass' 'pass' 'pass' 'buy' 'buy' 'pass' 'buy'\n",
      "   'pass' 'pass' 'buy']]]\n",
      "[[   7.19535395   10.96349798   22.54593293    6.4011521    49.12411005\n",
      "  -136.68076798    0.        ]]\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_names[action_log])\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a2c loss\n",
    "\n",
    "Here we define obective function for actor-critic (one-step) RL.\n",
    "\n",
    "* We regularize policy with expected inverse action probabilities (discouraging very small probas) to make objective numerically stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100, replace=True)\n",
    "\n",
    "_,_,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-394a16c7d7a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                        \u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                        \u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                        gamma_or_gammas=0.99)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#compute mean over \"alive\" fragments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manatee/anaconda3/lib/python3.6/site-packages/agentnet/learning/a2c.py\u001b[0m in \u001b[0;36mget_elementwise_objective\u001b[0;34m(policy, state_values, actions, rewards, is_alive, state_values_target, n_steps, n_steps_advantage, gamma_or_gammas, crop_last, state_values_target_after_end, state_values_after_end, consider_value_reference_constant, force_end_at_last_tick, return_separate, treat_policy_as_logpolicy, loss_function)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mstate_values_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mis_alive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import a2c\n",
    "\n",
    "\n",
    "#Train via actor-critic (see here - https://www.youtube.com/watch?v=KHZVXao4qXs)\n",
    "\n",
    "elwise_mse_loss = a2c.get_elementwise_objective(policy_seq,V_seq[:,:,0],\n",
    "                                                       replay.actions[0],\n",
    "                                                       replay.rewards,\n",
    "                                                       replay.is_alive,\n",
    "                                                       gamma_or_gammas=0.99)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "reg = T.mean((1./policy_seq).sum(axis=-1))\n",
    "loss += 0.001*reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.rmsprop(loss,weights,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_path=\"records/openaigym.video.0.15895.video000000.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bnn import bbpwrap, NormalApproximation,sample_output\n",
    "from lasagne.layers import InputLayer, DenseLayer, EmbeddingLayer\n",
    "import theano.tensor as T\n",
    "@bbpwrap(NormalApproximation())\n",
    "class BayesDenseLayer(DenseLayer):pass\n",
    "@bbpwrap(NormalApproximation())\n",
    "class BayesEmbLayer(EmbeddingLayer):pass\n",
    "\n",
    "from curiosity import compile_vime_reward\n",
    "\n",
    "\n",
    "# FIXME : cant see class attribute\n",
    "target_rho = 1\n",
    "class BNN:\n",
    "    curiosity=0.01\n",
    "    target_rho = 1\n",
    "    \n",
    "    l_state = InputLayer((None, state_size),name='state var')\n",
    "    l_action = InputLayer((None,), input_var=T.ivector())\n",
    "\n",
    "    l_action_emb = BayesEmbLayer(l_action, env.action_space.n, 3)    \n",
    "    \n",
    "    l_concat = lasagne.layers.concat([l_action_emb, l_state])\n",
    "    \n",
    "    l_dense = BayesDenseLayer(l_concat, num_units=50,\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    l_out = BayesDenseLayer(l_dense, num_units=state_size,\n",
    "                            nonlinearity=None)\n",
    "        \n",
    "    params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "    ###training###\n",
    "    pred_states = lasagne.layers.get_output(l_out)\n",
    "    next_states = T.matrix(\"next states\")\n",
    "    mse = lasagne.objectives.squared_error(pred_states, next_states).mean()\n",
    "    \n",
    "    #replace logposterior with simple regularization on rho cuz we're lazy\n",
    "    reg = sum([lasagne.objectives.squared_error(rho, target_rho).mean() \n",
    "              for rho in lasagne.layers.get_all_params(l_out, rho=True)])\n",
    "    \n",
    "    loss = mse+ 0.01*reg\n",
    "    \n",
    "    updates = lasagne.updates.adam(loss,params)\n",
    "    \n",
    "    train_step = theano.function([l_state.input_var,l_action.input_var,next_states],\n",
    "                                 loss,updates=updates)\n",
    "    \n",
    "    ###sample random sessions from pool###\n",
    "    observations, = replay.observations\n",
    "    actions, = replay.actions\n",
    "    observations_flat = observations[:,:-1].reshape((-1,)+tuple(observations.shape[2:]))\n",
    "    actions_flat = actions[:,:-1].reshape((-1,))\n",
    "    next_observations_flat = observations[:,1:].reshape((-1,)+tuple(observations.shape[2:]))\n",
    "    sample_from_pool = theano.function([],[observations_flat,actions_flat,next_observations_flat])\n",
    "\n",
    "    \n",
    "    ###curiosity reward### aka KL(qnew,qold)\n",
    "    get_vime_reward_elwise = compile_vime_reward(l_out,l_state,l_action,params,n_samples=10)\n",
    "    \n",
    "    vime_reward_ma = 10.\n",
    "    @staticmethod\n",
    "    def add_vime_reward(observations,actions,rewards,is_alive,h0):\n",
    "        assert isinstance(observations, np.ndarray)\n",
    "        observations_flat = observations[:,:-1].reshape((-1,)+observations.shape[2:]).astype('float32')\n",
    "        actions_flat = actions[:,:-1].reshape((-1,)).astype('int32')\n",
    "        next_observations_flat = observations[:,1:].reshape((-1,)+observations.shape[2:]).astype('float32')\n",
    "\n",
    "        vime_rewards = BNN.get_vime_reward_elwise(observations_flat,actions_flat,next_observations_flat)\n",
    "        vime_rewards = np.concatenate([vime_rewards.reshape(rewards[:,:-1].shape),\n",
    "                                       np.zeros_like(rewards[:,-1:]),], axis=1)\n",
    "        #normalize by moving average\n",
    "        BNN.vime_reward_ma = 0.99*BNN.vime_reward_ma + 0.01*vime_rewards.mean()\n",
    "        \n",
    "        surrogate_rewards = rewards + BNN.curiosity/BNN.vime_reward_ma*vime_rewards\n",
    "        return (observations,actions,surrogate_rewards,is_alive,h0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pre-fill pool\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    pool.update(SEQ_LENGTH,append=True,preprocess=BNN.add_vime_reward)\n",
    "\n",
    "#pre-train BNN (mitigate training lag on first iterations where BNN is stupid)\n",
    "for i in tqdm(range(1000)):\n",
    "    BNN.train_step(*BNN.sample_from_pool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "loss = 0\n",
    "for i in tqdm(range(1000)):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    for i in range(10):\n",
    "        pool.update(SEQ_LENGTH,append=True,preprocess=BNN.add_vime_reward)\n",
    "\n",
    "    for i in range(10):\n",
    "        loss = loss*0.99 + train_step()*0.01\n",
    "    \n",
    "    for i in range(10):\n",
    "        BNN.train_step(*BNN.sample_from_pool())\n",
    "\n",
    "    if epoch_counter%100==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = np.average(pool.experience_replay.rewards.get_value()[:,:-1],\n",
    "                                      weights=1+pool.experience_replay.is_alive.get_value()[:,:-1])\n",
    "        pool_size = pool.experience_replay.rewards.get_value().shape[0]\n",
    "        print(\"iter=%i\\treward/step=%.5f\\tpool_size=%i\\tvime ma=%.5f\"%(epoch_counter,\n",
    "                                                         pool_mean_reward,\n",
    "                                                         pool_size,\n",
    "                                                         BNN.vime_reward_ma))\n",
    "        \n",
    "\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        n_games = 10\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iters, session_rewards=zip(*sorted(rewards.items(),key=lambda pr:pr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iters,list(map(np.mean, session_rewards)))\n",
    "plt.title(\"Training progress\")\n",
    "plt.xlabel(\"Epoch counter\")\n",
    "plt.ylabel(\"Mean Income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,_,_,_,(pool_policy,pool_V) = agent.get_sessions(\n",
    "    pool.experience_replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,)\n",
    "\n",
    "states = pool.experience_replay.observations[0].get_value().reshape((-1, 5)).T[-2:]\n",
    "values = pool_V.ravel().eval()\n",
    "optimal_actid = pool_policy.argmax(-1).ravel().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    *states,\n",
    "    c=values,\n",
    "    alpha = 0.1)\n",
    "plt.title(\"predicted state values\")\n",
    "plt.xlabel(\"previous\")\n",
    "plt.ylabel(\"current\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs_x, obs_y = states\n",
    "\n",
    "for i in range(3):\n",
    "    sel = optimal_actid==i\n",
    "    plt.scatter(obs_x[sel],obs_y[sel],\n",
    "                c=['red','blue','green'][i],\n",
    "                alpha = 0.1,label=action_names[i])\n",
    "    \n",
    "plt.title(\"most likely action id\")\n",
    "plt.xlabel(\"previous\")\n",
    "plt.ylabel(\"current\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
